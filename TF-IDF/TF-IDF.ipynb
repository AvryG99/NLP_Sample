{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here compute the TF-IDF on a corpus of newspaper headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data into the file *headlines.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170721</td>\n",
       "      <td>algorithms can make decisions on behalf of fed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170721</td>\n",
       "      <td>andrew forrests fmg to appeal pilbara native t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170721</td>\n",
       "      <td>a rural mural in thallan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australia church risks becoming haven for abusers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australian company usgfx embroiled in shanghai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australia suffers shock loss in womens world c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20170721</td>\n",
       "      <td>big rigs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20170721</td>\n",
       "      <td>boy charged in connection with supermarket syr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20170721</td>\n",
       "      <td>breaking bad creator vince gilligan on success...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20170721</td>\n",
       "      <td>breaking bad creator vince gilligan on walter ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20170721  algorithms can make decisions on behalf of fed...\n",
       "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
       "2      20170721                           a rural mural in thallan\n",
       "3      20170721  australia church risks becoming haven for abusers\n",
       "4      20170721  australian company usgfx embroiled in shanghai...\n",
       "5      20170721  australia suffers shock loss in womens world c...\n",
       "6      20170721                                           big rigs\n",
       "7      20170721  boy charged in connection with supermarket syr...\n",
       "8      20170721  breaking bad creator vince gilligan on success...\n",
       "9      20170721  breaking bad creator vince gilligan on walter ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the dataset\n",
    "df = pd.read_csv('headlines.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, check the dataset basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       publish_date\n",
      "count  1.999000e+03\n",
      "mean   2.017069e+07\n",
      "std    3.926710e+01\n",
      "min    2.017062e+07\n",
      "25%    2.017063e+07\n",
      "50%    2.017071e+07\n",
      "75%    2.017072e+07\n",
      "max    2.017072e+07\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1999 entries, 0 to 1998\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   publish_date   1999 non-null   int64 \n",
      " 1   headline_text  1999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TODO: Have a look at the data\n",
    "print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform preprocessing on this text data: tokenization, punctuation and stop words removal and stemming.\n",
    "\n",
    "Hint: to do so, use NLTK, *pandas*'s method *apply*, lambda functions and list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Avry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Avry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                algorithm make decis behalf feder minist\n",
       "1       andrew forrest fmg appeal pilbara nativ titl rule\n",
       "2                                     rural mural thallan\n",
       "3                        australia church risk becom abus\n",
       "4       australian compani usgfx embroil shanghai staf...\n",
       "                              ...                        \n",
       "1994    constitut avenu win top prize act architectu a...\n",
       "1995                              dark mofo number crunch\n",
       "1996    david petraeu say australia must firm south ch...\n",
       "1997    driverless car australia face challeng roo pro...\n",
       "1998                     drug compani criticis price hike\n",
       "Name: processed_text, Length: 1999, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def process_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    # Stem words\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['processed_text'] = df['headline_text'].apply(process_text)\n",
    "df['processed_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute now the Bag of Words for our data, using scikit-learn.\n",
    "\n",
    "Warning: since we used our own preprocessing, you have to bypass analyzer with identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      10  100  1000km  100th  10m  10th  110kph  12  120  122  ...  youtub  \\\n",
      "0      0    0       0      0    0     0       0   0    0    0  ...       0   \n",
      "1      0    0       0      0    0     0       0   0    0    0  ...       0   \n",
      "2      0    0       0      0    0     0       0   0    0    0  ...       0   \n",
      "3      0    0       0      0    0     0       0   0    0    0  ...       0   \n",
      "4      0    0       0      0    0     0       0   0    0    0  ...       0   \n",
      "...   ..  ...     ...    ...  ...   ...     ...  ..  ...  ...  ...     ...   \n",
      "1994   0    0       0      0    0     0       0   0    0    0  ...       0   \n",
      "1995   0    0       0      0    0     0       0   0    0    0  ...       0   \n",
      "1996   0    0       0      0    0     0       0   0    0    0  ...       0   \n",
      "1997   0    0       0      0    0     0       0   0    0    0  ...       0   \n",
      "1998   0    0       0      0    0     0       0   0    0    0  ...       0   \n",
      "\n",
      "      zambian  zealand  zedd  zinc  zion  zombi  zone  zonta  zoo  \n",
      "0           0        0     0     0     0      0     0      0    0  \n",
      "1           0        0     0     0     0      0     0      0    0  \n",
      "2           0        0     0     0     0      0     0      0    0  \n",
      "3           0        0     0     0     0      0     0      0    0  \n",
      "4           0        0     0     0     0      0     0      0    0  \n",
      "...       ...      ...   ...   ...   ...    ...   ...    ...  ...  \n",
      "1994        0        0     0     0     0      0     0      0    0  \n",
      "1995        0        0     0     0     0      0     0      0    0  \n",
      "1996        0        0     0     0     0      0     0      0    0  \n",
      "1997        0        0     0     0     0      0     0      0    0  \n",
      "1998        0        0     0     0     0      0     0      0    0  \n",
      "\n",
      "[1999 rows x 4251 columns]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the BOW of the preprocessed data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the shape of the BOW, the expected value is `(1999, 4165)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the Term Frequency and then the Inverse Document Frequency, and check the values are not only zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the TF using the BOW\n",
    "tf_df = bow_df.div(bow_df.sum(axis=1), axis=0)\n",
    "\n",
    "tf = tf_df.to_numpy()\n",
    "tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.99146455, 7.2146081 , 7.90775528, ..., 7.90775528, 7.90775528,\n",
       "       7.90775528])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "\n",
    "# Fit the transformer on the BoW matrix to compute IDF\n",
    "tfidf_transformer.fit(X)\n",
    "\n",
    "# Get the IDF values\n",
    "idf_values = tfidf_transformer.idf_\n",
    "idf_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute finally the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: compute the TF-IDF\n",
    "tf_idf = tf * idf_values\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 10 words with the highest and lowest TF-IDF on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 highest:\n",
      "[('australia', 0.025560931184250436), ('australian', 0.025431119138871205), ('new', 0.021839611973342384), ('polic', 0.018525533232928312), ('market', 0.018459756901048795), ('say', 0.01822373349221039), ('trump', 0.016865140274321757), ('wa', 0.01598724728325081), ('man', 0.015695709222202744), ('sydney', 0.014814531550443662)]\n",
      "\n",
      "Top 10 lowest:\n",
      "[('syd', 0.00032965463060622547), ('nmfc', 0.00032965463060622547), ('melb', 0.00032965463060622547), ('haw', 0.00032965463060622547), ('gw', 0.00032965463060622547), ('geel', 0.00032965463060622547), ('gcfc', 0.00032965463060622547), ('coll', 0.00032965463060622547), ('adel', 0.00032965463060622547), ('nemtsov', 0.0003596232333886096)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
    "mean_tf_idf_scores = np.mean(tf_idf, axis=0)\n",
    "terms_with_scores = list(zip(vectorizer.get_feature_names_out(), mean_tf_idf_scores))\n",
    "sorted_terms = sorted(terms_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#10 highest words\n",
    "top_words = [(term, score) for term, score in sorted_terms if score != 0][:10]\n",
    "#10 lowest words\n",
    "bottom_words = [(term, score) for term, score in reversed(sorted_terms) if score != 0][:10]\n",
    "\n",
    "print(\"Top 10 highest:\")\n",
    "print(f'{top_words}\\n')\n",
    "print('Top 10 lowest:')\n",
    "print(bottom_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the TF-IDF using scikit-learn on our preprocessed data (the one you used to compute the BOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the processed text to compute TF-IDF\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# Convert TF-IDF matrix to a dense array for better readability\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Get feature names (terms)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame with TF-IDF scores\n",
    "df_tfidf = pd.DataFrame(tfidf_array, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000km</th>\n",
       "      <th>100th</th>\n",
       "      <th>10m</th>\n",
       "      <th>10th</th>\n",
       "      <th>110kph</th>\n",
       "      <th>12</th>\n",
       "      <th>120</th>\n",
       "      <th>122</th>\n",
       "      <th>...</th>\n",
       "      <th>youtub</th>\n",
       "      <th>zambian</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zedd</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zonta</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1999 rows × 4251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       10  100  1000km  100th  10m  10th  110kph   12  120  122  ...  youtub  \\\n",
       "0     0.0  0.0     0.0    0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...     0.0   \n",
       "1     0.0  0.0     0.0    0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...     0.0   \n",
       "2     0.0  0.0     0.0    0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...     0.0   \n",
       "3     0.0  0.0     0.0    0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...     0.0   \n",
       "4     0.0  0.0     0.0    0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...     0.0   \n",
       "...   ...  ...     ...    ...  ...   ...     ...  ...  ...  ...  ...     ...   \n",
       "1994  0.0  0.0     0.0    0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...     0.0   \n",
       "1995  0.0  0.0     0.0    0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...     0.0   \n",
       "1996  0.0  0.0     0.0    0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...     0.0   \n",
       "1997  0.0  0.0     0.0    0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...     0.0   \n",
       "1998  0.0  0.0     0.0    0.0  0.0   0.0     0.0  0.0  0.0  0.0  ...     0.0   \n",
       "\n",
       "      zambian  zealand  zedd  zinc  zion  zombi  zone  zonta  zoo  \n",
       "0         0.0      0.0   0.0   0.0   0.0    0.0   0.0    0.0  0.0  \n",
       "1         0.0      0.0   0.0   0.0   0.0    0.0   0.0    0.0  0.0  \n",
       "2         0.0      0.0   0.0   0.0   0.0    0.0   0.0    0.0  0.0  \n",
       "3         0.0      0.0   0.0   0.0   0.0    0.0   0.0    0.0  0.0  \n",
       "4         0.0      0.0   0.0   0.0   0.0    0.0   0.0    0.0  0.0  \n",
       "...       ...      ...   ...   ...   ...    ...   ...    ...  ...  \n",
       "1994      0.0      0.0   0.0   0.0   0.0    0.0   0.0    0.0  0.0  \n",
       "1995      0.0      0.0   0.0   0.0   0.0    0.0   0.0    0.0  0.0  \n",
       "1996      0.0      0.0   0.0   0.0   0.0    0.0   0.0    0.0  0.0  \n",
       "1997      0.0      0.0   0.0   0.0   0.0    0.0   0.0    0.0  0.0  \n",
       "1998      0.0      0.0   0.0   0.0   0.0    0.0   0.0    0.0  0.0  \n",
       "\n",
       "[1999 rows x 4251 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the 10 highest and lowest TF-IDF words on average to the ones you had by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words with highest TF-IDF scores:\n",
      "peacemak     1.000000\n",
      "pump         1.000000\n",
      "mongolian    0.831769\n",
      "financ       0.803629\n",
      "employ       0.795060\n",
      "date         0.794899\n",
      "aquapon      0.794899\n",
      "travel       0.788050\n",
      "rig          0.786813\n",
      "mosul        0.779137\n",
      "dtype: float64\n",
      "\n",
      "Bottom 10 words with lowest TF-IDF scores (excluding score 0):\n",
      "10          0.0\n",
      "place       0.0\n",
      "placemak    0.0\n",
      "plagu       0.0\n",
      "plain       0.0\n",
      "plan        0.0\n",
      "plane       0.0\n",
      "plant       0.0\n",
      "plastic     0.0\n",
      "plate       0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 words with highest TF-IDF scores\n",
    "top_words = df_tfidf.max().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Find bottom 10 words with lowest TF-IDF scores (excluding words with score 0)\n",
    "# Filter out words where all TF-IDF scores are 0\n",
    "bottom_words = df_tfidf.loc[:, (df_tfidf != 0.0).any()].min().sort_values().astype(float).head(10)\n",
    "\n",
    "print(\"Top 10 words with highest TF-IDF scores:\")\n",
    "print(top_words)\n",
    "print(\"\\nBottom 10 words with lowest TF-IDF scores (excluding score 0):\")\n",
    "print(bottom_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have the same words? How do you explain it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
